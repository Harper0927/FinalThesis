{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b484d0",
   "metadata": {
    "id": "79b484d0"
   },
   "source": [
    "## Crawl Goodreads Book Pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e2b9b7e",
   "metadata": {
    "id": "8e2b9b7e",
    "outputId": "88d1015a-15d1-43c1-ffb8-6f1ffe796bc0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_romance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.goodreads.com/book/show/61718053-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.goodreads.com/book/show/61326735-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.goodreads.com/book/show/61918816-y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         URL_romance\n",
       "0  https://www.goodreads.com/book/show/61718053-h...\n",
       "1  https://www.goodreads.com/book/show/61326735-l...\n",
       "2  https://www.goodreads.com/book/show/61918816-y..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# The core dataset with book titles, ISBNs and Goodreads URLs\n",
    "tekla_file = '1_Romance books_top 20 Goodreads 2011-2023 - Sheet1.csv'\n",
    "\n",
    "# The output directory\n",
    "html_dir = 'reviews/'\n",
    "\n",
    "# Read the core dataset as a pandas DataFrame and show the Goodreads link column\n",
    "source_df = pd.read_csv(tekla_file)[0:3]\n",
    "source_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595338f2",
   "metadata": {
    "id": "595338f2"
   },
   "source": [
    "Next, download and store the book pages for all URLs in the core dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec22dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m147.9/147.9 kB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d80cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>reviews_count</th>\n",
       "      <th>description</th>\n",
       "      <th>genres</th>\n",
       "      <th>pages_format</th>\n",
       "      <th>publication_info</th>\n",
       "      <th>ratings_histogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy Place</td>\n",
       "      <td>Emily Henry</td>\n",
       "      <td>4.00</td>\n",
       "      <td>917,038 ratings</td>\n",
       "      <td>110,010 reviews</td>\n",
       "      <td>Harriet and Wyn have been the perfect couple s...</td>\n",
       "      <td>[Romance, Fiction, Contemporary, Audiobook, Co...</td>\n",
       "      <td>400 pages, Hardcover</td>\n",
       "      <td>First published April 25, 2023</td>\n",
       "      <td>[310,324 (33%), 361,439 (39%), 192,186 (20%), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love, Theoretically</td>\n",
       "      <td>Ali Hazelwood</td>\n",
       "      <td>4.11</td>\n",
       "      <td>416,471 ratings</td>\n",
       "      <td>58,046 reviews</td>\n",
       "      <td>The many lives of theoretical physicist Elsie ...</td>\n",
       "      <td>[Romance, Contemporary, Contemporary Romance, ...</td>\n",
       "      <td>389 pages, Paperback</td>\n",
       "      <td>First published June 13, 2023</td>\n",
       "      <td>[153,411 (36%), 175,880 (42%), 71,585 (17%), 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yours Truly</td>\n",
       "      <td>Abby Jimenez</td>\n",
       "      <td>4.33</td>\n",
       "      <td>358,447 ratings</td>\n",
       "      <td>48,314 reviews</td>\n",
       "      <td>A novel of terrible first impressions, hilario...</td>\n",
       "      <td>[Romance, Fiction, Contemporary, Audiobook, Co...</td>\n",
       "      <td>416 pages, Paperback</td>\n",
       "      <td>First published April 11, 2023</td>\n",
       "      <td>[171,389 (47%), 139,921 (39%), 40,530 (11%), 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title         author average_rating    ratings_count  \\\n",
       "0          Happy Place    Emily Henry           4.00  917,038 ratings   \n",
       "1  Love, Theoretically  Ali Hazelwood           4.11  416,471 ratings   \n",
       "2          Yours Truly   Abby Jimenez           4.33  358,447 ratings   \n",
       "\n",
       "     reviews_count                                        description  \\\n",
       "0  110,010 reviews  Harriet and Wyn have been the perfect couple s...   \n",
       "1   58,046 reviews  The many lives of theoretical physicist Elsie ...   \n",
       "2   48,314 reviews  A novel of terrible first impressions, hilario...   \n",
       "\n",
       "                                              genres          pages_format  \\\n",
       "0  [Romance, Fiction, Contemporary, Audiobook, Co...  400 pages, Hardcover   \n",
       "1  [Romance, Contemporary, Contemporary Romance, ...  389 pages, Paperback   \n",
       "2  [Romance, Fiction, Contemporary, Audiobook, Co...  416 pages, Paperback   \n",
       "\n",
       "                 publication_info  \\\n",
       "0  First published April 25, 2023   \n",
       "1   First published June 13, 2023   \n",
       "2  First published April 11, 2023   \n",
       "\n",
       "                                   ratings_histogram  \n",
       "0  [310,324 (33%), 361,439 (39%), 192,186 (20%), ...  \n",
       "1  [153,411 (36%), 175,880 (42%), 71,585 (17%), 1...  \n",
       "2  [171,389 (47%), 139,921 (39%), 40,530 (11%), 5...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Define sleep time to prevent banning from server\n",
    "def sleep():\n",
    "    sleep_time = 10 + random.randint(0, 10) + random.random()\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = source_df.URL_romance.to_list()\n",
    "\n",
    "data = []\n",
    "\n",
    "for url in urls:\n",
    "     try:\n",
    "         # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract metadata\n",
    "        title = soup.find(\"h1\", attrs={\"data-testid\":\"bookTitle\"}).text.strip()\n",
    "        author = soup.find(\"span\", attrs={\"data-testid\":\"name\"}).text.strip()\n",
    "        average_rating = soup.find(\"div\", attrs={\"class\":\"RatingStatistics__rating\"}).text.strip()\n",
    "        ratings_count = soup.find(\"span\", attrs={\"data-testid\":\"ratingsCount\"}).text.strip()\n",
    "        reviews_count = soup.find(\"span\", attrs={\"data-testid\":\"reviewsCount\"}).text.strip()\n",
    "        description = soup.find(\"div\", attrs={\"data-testid\":\"description\"}).text.strip()\n",
    "        genres = [genre.text.strip() for genre in soup.find_all(\"span\", attrs={\"class\":\"BookPageMetadataSection__genreButton\"})]\n",
    "        pages_format = soup.find(\"p\", attrs={\"data-testid\":\"pagesFormat\"}).text.strip()\n",
    "        publication_info = soup.find(\"p\", attrs={\"data-testid\":\"publicationInfo\"}).text.strip()\n",
    "        #currently_reading_signal = soup.find(\"div\", attrs={\"data-testid\":\"currentlyReadingSignal\"}).text.strip()\n",
    "        #to_read_signal = soup.find(\"div\", attrs={\"data-testid\":\"toReadSignal\"}).text.strip()\n",
    "        #awards = [award.text for award in soup.select('div.DescListItem dd a')]\n",
    "        #isbn = soup.find('dt', string='ISBN').find_next(\"div\", attrs={\"data-testid\":\"contentContainer\"}).text.strip()\n",
    "        ratings_histogram = [rating.text.strip() for rating in soup.find_all(\"div\", attrs={\"class\":\"RatingsHistogram__labelTotal\"})]\n",
    "\n",
    "        # Append the extracted data to the list\n",
    "        data.append({\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"average_rating\": average_rating,\n",
    "            \"ratings_count\": ratings_count,\n",
    "            \"reviews_count\": reviews_count,\n",
    "            \"description\": description,\n",
    "            \"genres\": genres,\n",
    "            \"pages_format\": pages_format,\n",
    "            \"publication_info\": publication_info,\n",
    "            #\"currently_reading_signal\": currently_reading_signal,\n",
    "            #\"to_read_signal\": to_read_signal,\n",
    "            #\"awards\": awards,\n",
    "            #\"isbn\": isbn,\n",
    "            \"ratings_histogram\": ratings_histogram\n",
    "        })\n",
    "        \n",
    "     except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Skipping URL: {url}\")\n",
    "        continue\n",
    "\n",
    "        sleep()\n",
    "\n",
    "# Create a dataframe from the list\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f90875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"test_books_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6b626",
   "metadata": {},
   "source": [
    "# Scrape reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ce63cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# !!! Adapt this to scrape reviews for a list of URLs\n",
    "url = \"https://www.goodreads.com/book/show/61718053/reviews\"\n",
    "url = \"https://book.douban.com/subject/2567698/comments/\"\n",
    "\n",
    "# Set up Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Open Goodreads website\n",
    "driver.get(url)\n",
    "proxy = {'https':'http://127.0.0.1:2802', 'http':'http://127.0.0.1:2802'}#国内得翻墙，加入了VPN代理的Http端口\n",
    "# Click on the \"Load more reviews\" button until all reviews are loaded\n",
    "# !!!! This part need to be fixed to make sure you load more reviews beyond the first 30 displayed on a page !!!\n",
    "click_count = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#Goodreads\n",
    "while True:\n",
    "    try:\n",
    "        load_more_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"[data-testid='loadMore']\")))\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(load_more_button).perform()\n",
    "        load_more_button.click()\n",
    "        time.sleep(2)  # Wait for the next batch of reviews to load\n",
    "        click_count += 1\n",
    "        if click_count >= 10:\n",
    "            break\n",
    "    except:\n",
    "        break  # If the \"Load more reviews\" button is not found, assume all reviews are loaded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Anobii\n",
    "# Get the HTML content\n",
    "html = requests.get(url,proxies=proxy)\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "ht_test = html.text\n",
    "page_count = 0\n",
    "comment_InfoList = []\n",
    "Booktitle = driver.find_element(By.CLASS_NAME,'unstyled-link').text\n",
    "SavePath = 'Book_'+Booktitle + '.csv'\n",
    "while page_count<10 :\n",
    "    # 获取reviews-list中的所有activity-item-review项目\n",
    "    reviews_list = driver.find_element(By.CLASS_NAME, 'reviews-list')\n",
    "    reviews = reviews_list.find_elements(By.TAG_NAME, 'app-activity-item-review')\n",
    "\n",
    "    for review in reviews:\n",
    "        comment_content = review.find_element(By.CLASS_NAME, 'text_overflow').text\n",
    "        try:\n",
    "            comment_likes = review.find_element(By.CLASS_NAME, 'anchor').text\n",
    "        except NoSuchElementException:\n",
    "             comment_likes = 0\n",
    "        comment_InfoList.append({\n",
    "                \"comment_content\": comment_content,\n",
    "                \"comment_likes\": comment_likes,\n",
    "            })\n",
    "    # 尝试找到翻页按钮并点击\n",
    "    try:\n",
    "        # 定位下一页按钮\n",
    "        next_button = driver.find_element(By.XPATH, \"//li[@class='page-item ng-star-inserted']/a[@aria-label='Next']\")\n",
    "        # 使用 JavaScript 来点击元素\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        page_count += 1\n",
    "        driver.implicitly_wait(10)  # 等待新页面加载\n",
    "    except NoSuchElementException:\n",
    "        break  # 没有找到翻页按钮，结束循环\n",
    "df = pd.DataFrame(comment_InfoList)\n",
    "df.to_csv(SavePath,encoding='utf-8-sig',index=False)\n",
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Goodreads改\n",
    "# Get the HTML content\n",
    "html = requests.get(url,proxies=proxy)\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "ht_test = html.text\n",
    "click_count = 0\n",
    "comment_InfoList = []\n",
    "Booktitle = soup.find(\"a\", attrs={\"data-testid\":\"title\"}).text.strip()\n",
    "SavePath = 'Book_'+Booktitle + '.csv'\n",
    "while click_count<20 :\n",
    "    try:\n",
    "        load_more_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"[data-testid='loadMore']\")))\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(load_more_button).perform()\n",
    "        load_more_button.click()\n",
    "        time.sleep(5)  # Wait for the next batch of reviews to load\n",
    "        click_count += 1\n",
    "        if click_count >= 20:\n",
    "            break\n",
    "    except NoSuchElementException:\n",
    "        break  # 没有找到翻页按钮，结束循环\n",
    "    # 获取reviews-list中的所有activity-item-review项目\n",
    "reviews_list = driver.find_element(By.CLASS_NAME, 'ReviewsList')\n",
    "reviews = reviews_list.find_elements(By.CLASS_NAME, 'ReviewCard__content')\n",
    "for review in reviews:\n",
    "    comment_content = review.find_element(By.CLASS_NAME, 'ReviewText').text\n",
    "    try:\n",
    "        comment_rating = (review.find_element(By.CLASS_NAME,'ShelfStatus').find_element(By.TAG_NAME,'span').get_attribute('aria-label').split())[1]\n",
    "    except:\n",
    "        comment_rating=0\n",
    "    try:\n",
    "        comment_likes = (review.find_element(By.CLASS_NAME, 'SocialFooter').find_element(By.CLASS_NAME, 'Button__labelItem').text.split())[0]\n",
    "    except:\n",
    "        comment_likes=0\n",
    "\n",
    "    comment_InfoList.append({\n",
    "            \"comment_content\": comment_content,\n",
    "            \"comment_rating\":comment_rating,\n",
    "            \"comment_likes\": comment_likes,\n",
    "        })\n",
    "df = pd.DataFrame(comment_InfoList)\n",
    "df.to_csv(SavePath,encoding='utf-8-sig',index=False)\n",
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#在需要登陆的网站需要先获取cookies\n",
    "import json\n",
    "# 配置第二个 Chrome 选项\n",
    "options_2 = Options()\n",
    "options_2.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "driver2 = webdriver.Chrome(options=options_2)\n",
    "try:\n",
    "    # 打开小红书主页并手动登录\n",
    "    driver2.get(url)\n",
    "    time.sleep(60)  # 给用户足够的时间手动登录\n",
    "\n",
    "    # 获取登录后的 cookie\n",
    "    cookies = driver.get_cookies()\n",
    "\n",
    "    # 打印当前工作目录\n",
    "    print(\"Current working directory: \", os.getcwd())\n",
    "    # 将 cookie 保存到文件\n",
    "    with open(\"cookies.json\", \"w\") as file:\n",
    "        json.dump(cookies, file)\n",
    "    print(\"Cookies saved successfully.\")\n",
    "finally:\n",
    "    driver2.quit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#豆瓣\n",
    "options_2 = Options()\n",
    "options_2.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "driver2 = webdriver.Chrome(options=options_2)\n",
    "try:\n",
    "    # 打开并手动登录\n",
    "    driver2.get(url)\n",
    "    time.sleep(60)  # 给用户足够的时间手动登录\n",
    "    driver2.refresh()\n",
    "except:\n",
    "    printf('err')\n",
    "\n",
    "comment_InfoList = []\n",
    "Booktitle = driver2.find_element(By.ID,'content').find_element(By.TAG_NAME,'h1').text\n",
    "SavePath = 'Book_'+Booktitle + '_douban.csv'\n",
    "rating_array=['很差','较差','还行','推荐','力荐']#评分数组，用于获取对应评分title对应的索引rating\n",
    "\n",
    "while comment_InfoList.__len__()<810 :\n",
    "    reviews_list = driver2.find_element(By.CLASS_NAME, 'comment-list')\n",
    "    reviews = reviews_list.find_elements(By.CLASS_NAME, 'comment-item')\n",
    "    for review in reviews:\n",
    "        comment_content = review.find_element(By.CLASS_NAME, 'comment-content').text\n",
    "        try:\n",
    "            rate_s = review.find_element(By.CLASS_NAME,'comment-info').find_element(By.TAG_NAME,'span').get_attribute('title')\n",
    "            for index,string in enumerate(rating_array):\n",
    "                if string == rate_s:\n",
    "                    comment_rating = index+1\n",
    "                    break\n",
    "        except:\n",
    "            comment_rating=0\n",
    "        try:\n",
    "            comment_likes = review.find_element(By.CLASS_NAME, 'vote-count').text\n",
    "        except:\n",
    "            comment_likes=0\n",
    "\n",
    "        comment_InfoList.append({\n",
    "                \"comment_content\": comment_content,\n",
    "                \"comment_rating\":comment_rating,\n",
    "                \"comment_likes\": comment_likes,\n",
    "            })\n",
    "    try:\n",
    "        # 定位下一页按钮\n",
    "        page_buttons = driver2.find_element(By.ID,'paginator').find_elements(By.TAG_NAME, 'a')\n",
    "        for button in page_buttons:\n",
    "            if button.text == '后页 >':\n",
    "                next_button=button\n",
    "                break\n",
    "        # 使用 JavaScript 来点击元素\n",
    "        driver2.execute_script(\"arguments[0].click();\", next_button)\n",
    "        driver2.implicitly_wait(10)  # 等待新页面加载\n",
    "    except NoSuchElementException:\n",
    "        break  # 没有找到翻页按钮，结束循环\n",
    "df = pd.DataFrame(comment_InfoList)\n",
    "df.to_csv(SavePath,encoding='utf-8-sig',index=False)\n",
    "driver2.quit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".comment-list\"}\n  (Session info: chrome=127.0.6533.120)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNoSuchElementException\u001B[0m                    Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-18-93530974490e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;32mwhile\u001B[0m \u001B[0mcomment_InfoList\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__len__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m<\u001B[0m\u001B[1;36m810\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m     \u001B[0mreviews_list\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdriver2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind_element\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mCLASS_NAME\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'comment-list'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m     \u001B[0mreviews\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mreviews_list\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind_elements\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mCLASS_NAME\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'comment-item'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mreview\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mreviews\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python36\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001B[0m in \u001B[0;36mfind_element\u001B[1;34m(self, by, value)\u001B[0m\n\u001B[0;32m    976\u001B[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001B[0;32m    977\u001B[0m             \u001B[1;34m'using'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mby\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 978\u001B[1;33m             'value': value})['value']\n\u001B[0m\u001B[0;32m    979\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    980\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfind_elements\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mby\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mBy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mID\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\program files\\python36\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001B[0m in \u001B[0;36mexecute\u001B[1;34m(self, driver_command, params)\u001B[0m\n\u001B[0;32m    319\u001B[0m         \u001B[0mresponse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcommand_executor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecute\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdriver_command\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    320\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 321\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheck_response\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    322\u001B[0m             response['value'] = self._unwrap_value(\n\u001B[0;32m    323\u001B[0m                 response.get('value', None))\n",
      "\u001B[1;32mc:\\program files\\python36\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001B[0m in \u001B[0;36mcheck_response\u001B[1;34m(self, response)\u001B[0m\n\u001B[0;32m    240\u001B[0m                 \u001B[0malert_text\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'alert'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'text'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    241\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mexception_class\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscreen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstacktrace\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malert_text\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 242\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mexception_class\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscreen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstacktrace\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    243\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    244\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_value_or_default\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdefault\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNoSuchElementException\u001B[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".comment-list\"}\n  (Session info: chrome=127.0.6533.120)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Find all review containers and create a list to store the review texts\n",
    "\n",
    "#goodreads 的 class\n",
    "#review_texts = [review.text.strip() for review in soup.find_all(\"section\", attrs={\"class\":\"ReviewText__content\"})]\n",
    "\n",
    "#gr\n",
    "test_sec = soup.find_all('app-activity-item-review',attrs={\"class\":\"text_overflow\"})\n",
    "text_test = soup.find_all(\"section\", attrs={\"class\":\"text_overflow\"})\n",
    "review_texts = [review.text.strip() for review in soup.find_all(\"section\", attrs={\"class\":\"text_overflow\"})]#goodreads 的 class\n",
    "\n",
    "# Create a Pandas DataFrame from the review texts\n",
    "df = pd.DataFrame({'review_text': review_texts})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('test_goodreads_reviews.csv' ,index=False)\n",
    "\n",
    "# Close the Chrome driver\n",
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c6bf025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to test with just one review and .find()\n",
    "#dct = {'ReviewText_content': review_texts}\n",
    "#dct = {k:[v] for k,v in dct.items()}\n",
    "#df = pd.DataFrame(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50acc386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [review_text]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_text</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}